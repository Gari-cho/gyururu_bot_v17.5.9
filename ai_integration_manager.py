#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
==========================================================
ğŸ§© Gyururu Bot File Metadata
==========================================================
ä½œæˆæ—¥æ™‚: 2025-11-10
å¯¾å¿œãƒãƒ¼ã‚¸ãƒ§ãƒ³: v17.3
ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª / ãƒ•ã‚¡ã‚¤ãƒ«å: ./ai_integration_manager.py
ãƒ•ã‚¡ã‚¤ãƒ«ã®å½¹å‰²:
  - ã€ŒAIã®å°ç·šã€çµ±åˆãƒãƒãƒ¼ã‚¸ãƒ£ï¼ˆOneComme/Chat â†’ AI â†’ Chat/Voiceï¼‰
  - v17.3 ã®æœ€å°å›è·¯: AI_REQUEST/ONECOMME_COMMENT/CHAT_MESSAGE ã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘ã€
    AI_RESPONSE/CHAT_APPEND ã‚’å‡ºåŠ›ï¼ˆå¿…è¦ã«å¿œã˜ VOICE_REQUEST ã‚‚ç™ºè¡Œï¼‰

ä¸»ãªæ©Ÿèƒ½:
  - MessageBus ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã¸ã®è‡ªå‹•æ¥ç¶šï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒã‚¹å†…è”µï¼‰
  - UnifiedConfigManager ã‹ã‚‰ AI è¨­å®šã‚’è‡ªå‹•èª­è¾¼
  - Gemini / Local-Echo ã® 2 ç³»çµ±ã‚³ãƒã‚¯ã‚¿ã‚’ç®¡ç†
  - ONECOMME_COMMENT / CHAT_MESSAGE ã‹ã‚‰ AI_REQUEST ã¸ã®æ©‹æ¸¡ã—
  - AI_RESPONSE / CHAT_APPEND / VOICE_REQUEST ã®é€å‡º

æ³¨æ„äº‹é …:
  - v17.3.1 ã§ã¯ã€Œå°ç·šãŒç”Ÿãã¦ã„ã‚‹ã€ã“ã¨ã‚’æœ€å„ªå…ˆã¨ã—ã€
    API ã‚­ãƒ¼ãŒç„¡ã„å ´åˆã§ã‚‚ Local-Echo ã§å¿…ãšå¿œç­”ã‚’è¿”ã™ã€‚
"""

from __future__ import annotations

import os
import time
import logging
from dataclasses import dataclass
from typing import Optional, Callable, Any, Dict

logger = logging.getLogger(__name__)

# ---------------------------------------------------------
# .env èª­è¾¼ãƒ˜ãƒ«ãƒ‘ï¼ˆå˜ç‹¬å®Ÿè¡Œæ™‚ã®ä¿é™ºï¼‰
# ---------------------------------------------------------
def _load_env_if_exists():
    """
    v17.3 æ¨™æº–: .env ãŒã‚ã‚Œã°èª­ã¿è¾¼ã‚€ï¼ˆã‚¨ãƒ©ãƒ¼ã¯æ¡ã‚Šã¤ã¶ã™ï¼‰
    """
    try:
        from pathlib import Path
        env_path = Path(".env")
        if env_path.exists():
            try:
                from dotenv import load_dotenv
                load_dotenv(env_path, override=True)
                logger.info(f"ğŸŒ .env èª­è¾¼å®Œäº†: {env_path}")
            except Exception:
                with open(env_path, "r", encoding="utf-8") as f:
                    for line in f:
                        line = line.strip()
                        if not line or line.startswith("#") or "=" not in line:
                            continue
                        k, v = line.split("=", 1)
                        os.environ[k.strip()] = v.strip()
                logger.info(f"ğŸŒ .envï¼ˆç°¡æ˜“ï¼‰èª­è¾¼å®Œäº†: {env_path}")
        else:
            logger.warning("âš ï¸ .env ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆç’°å¢ƒå¤‰æ•°ã®ã¿ä½¿ç”¨ï¼‰")
    except Exception as e:
        logger.warning(f"âš ï¸ .env èª­è¾¼å¤±æ•—: {e}")

_load_env_if_exists()

# ---------------------------------------------------------
# ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ãƒŸãƒ‹ MessageBus
# ---------------------------------------------------------
class _MiniBus:
    """
    shared.message_bus ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã¨ãç”¨ã®ç°¡æ˜“å®Ÿè£…ã€‚
    """
    def __init__(self):
        self._subs: Dict[str, list[Callable[[dict, Optional[str]], None]]] = {}

    def _key(self, ev) -> str:
        if isinstance(ev, str):
            return ev
        return str(ev)

    def publish(self, ev, data=None, sender=None):
        k = self._key(ev)
        if k not in self._subs:
            return
        for cb in list(self._subs[k]):
            try:
                cb(data or {}, sender)
            except Exception:
                logger.exception(f"MiniBus handler error: {cb}")

    def is_alive(self) -> bool:
        # v17.3: ãƒã‚¹ç”Ÿæ­»ã®ç›£è¦–ã§åˆ©ç”¨
        try:
            _ = bool(self._subs)
        except Exception:
            return False
        return True

    def subscribe(self, ev, cb):
        k = self._key(ev)
        self._subs.setdefault(k, []).append(cb)
        # ãƒˆãƒ¼ã‚¯ãƒ³ä»£ã‚ã‚Šã« (event, cb) ã‚’è¿”ã™
        return (k, cb)

    def unsubscribe(self, token_or_event, cb=None):
        try:
            if isinstance(token_or_event, tuple) and cb is None:
                k, fn = token_or_event
                if k in self._subs and fn in self._subs[k]:
                    self._subs[k].remove(fn)
            else:
                k = self._key(token_or_event)
                if k in self._subs and cb in self._subs[k]:
                    self._subs[k].remove(cb)
        except Exception:
            pass

def _get_bus():
    """
    shared.message_bus ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã€‚ç„¡ã‘ã‚Œã° _MiniBus ã‚’è¿”ã™ã€‚
    """
    try:
        # v17.3 æ¨å¥¨: ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³å–å¾—é–¢æ•°ã‚’åˆ©ç”¨
        from shared.message_bus import get_message_bus
        return get_message_bus()
    except Exception:
        try:
            # ç›´ä¸‹é…ç½®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            from message_bus import get_message_bus as _get_bus_fallback
            return _get_bus_fallback()
        except Exception:
            logger.warning("âš ï¸ MessageBus ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ _MiniBus ã‚’ä½¿ç”¨ã—ã¾ã™")
            return _MiniBus()

# ---------------------------------------------------------
# UnifiedConfigManager äº’æ›ãƒ˜ãƒ«ãƒ‘
# ---------------------------------------------------------
class _DummyConfig(dict):
    """
    ConfigManager ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã¨ãã®ç°¡æ˜“è¾æ›¸ã€‚
    get/set ã ã‘å¯¾å¿œã™ã‚Œã°ååˆ†ã€‚
    """
    def get(self, key, default=None):
        return super().get(key, default)

    def set(self, key, value):
        self[key] = value

def _get_config():
    """
    v17.3.1: ConfigManager ã¯ UnifiedConfigManager / get_config_manager() ã«çµ±ä¸€ã€‚
    - ã¾ãš unified_config_manager.py ã‹ã‚‰ get_config_manager ã‚’å–å¾—
    - å¤±æ•—ã—ãŸå ´åˆã¯ UnifiedConfigManager() ã‚’ç›´æ¥ç”Ÿæˆ
    - ã©ã¡ã‚‰ã‚‚ãƒ€ãƒ¡ãªå ´åˆã¯ {} ã‚’è¿”ã™ï¼ˆã‚¨ãƒ©ãƒ¼ã¯ãƒ­ã‚°ã«æ®‹ã™ï¼‰
    """
    try:
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç›´ä¸‹ãƒ‘ã‚¿ãƒ¼ãƒ³
        from unified_config_manager import get_config_manager, UnifiedConfigManager  # type: ignore
    except Exception:
        try:
            # sharedé…ä¸‹ã«ã‚ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
            from shared.unified_config_manager import get_config_manager, UnifiedConfigManager  # type: ignore
        except Exception as e:
            logger.error(f"âŒ UnifiedConfigManager ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return {}

    # get_config_manager() å„ªå…ˆ
    try:
        cfg = get_config_manager()
        if cfg is not None:
            return cfg
    except Exception as e:
        logger.error(f"âŒ get_config_manager() å‘¼ã³å‡ºã—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç›´æ¥ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ
    try:
        return UnifiedConfigManager()
    except Exception as e:
        logger.error(f"âŒ UnifiedConfigManager() ç”Ÿæˆã«ã‚‚å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return {}

# ---------------------------------------------------------
# ã‚¤ãƒ™ãƒ³ãƒˆç¨®åˆ¥ï¼ˆshared.event_types ã¨ã®é€£æºï¼‰
# ---------------------------------------------------------
try:
    from shared.event_types import Events as _Events
    Events = _Events
except Exception:
    class _CompatEvents:
        APP_STARTED       = "APP_STARTED"
        AI_REQUEST        = "AI_REQUEST"
        ONECOMME_COMMENT  = "ONECOMME_COMMENT"
        CHAT_MESSAGE      = "CHAT_MESSAGE"
        AI_RESPONSE       = "AI_RESPONSE"
        CHAT_APPEND       = "CHAT_APPEND"
        VOICE_REQUEST     = "VOICE_REQUEST"
        STATUS_UPDATE     = "STATUS_UPDATE"
        AI_ERROR          = "AI_ERROR"
        # v17.3.1 è¿½åŠ ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
        AI_STATUS_REQUEST = "AI_STATUS_REQUEST"
        AI_STATUS_UPDATE  = "AI_STATUS_UPDATE"
        AI_TEST_REQUEST   = "AI_TEST_REQUEST"
        CONFIG_UPDATE     = "CONFIG_UPDATE"
    Events = _CompatEvents()  # type: ignore

def _ev(name: str) -> str:
    return getattr(Events, name, name)

# ---------------------------------------------------------
# ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã‚µãƒ‹ã‚¿ã‚¤ã‚º
# ---------------------------------------------------------
def _sanitize_error_message(error_msg: str) -> str:
    """
    API ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰æ©Ÿå¯†æƒ…å ±ï¼ˆAPI ã‚­ãƒ¼ã€ãƒˆãƒ¼ã‚¯ãƒ³ãªã©ï¼‰ã‚’é™¤å»ã€‚
    """
    import re
    msg = str(error_msg)

    # API ã‚­ãƒ¼ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ§˜ã€…ãªå½¢å¼ã«å¯¾å¿œï¼‰
    # Example: "API_KEY=AIza..." â†’ "API_KEY=***"
    msg = re.sub(r'(api[_-]?key\s*[=:]\s*)["\']?[\w\-]{20,}["\']?', r'\1***', msg, flags=re.IGNORECASE)

    # Bearer ãƒˆãƒ¼ã‚¯ãƒ³
    msg = re.sub(r'(bearer\s+)[\w\-\.]+', r'\1***', msg, flags=re.IGNORECASE)

    # Authorization ãƒ˜ãƒƒãƒ€ãƒ¼
    msg = re.sub(r'(authorization\s*:\s*)[^\s,}]+', r'\1***', msg, flags=re.IGNORECASE)

    # é•·ã„è‹±æ•°å­—æ–‡å­—åˆ—ï¼ˆ40æ–‡å­—ä»¥ä¸Šã®é€£ç¶šã—ãŸè‹±æ•°å­—ã¯ã‚­ãƒ¼ã®å¯èƒ½æ€§ï¼‰
    msg = re.sub(r'\b[A-Za-z0-9_\-]{40,}\b', '***', msg)

    return msg

# ---------------------------------------------------------
# AI ã‚³ãƒã‚¯ã‚¿ï¼ˆã‚ªãƒ•ãƒ©ã‚¤ãƒ³å®‰å…¨å®Ÿè£…ï¼‰
# ---------------------------------------------------------
@dataclass
class AIConnectorResult:
    text: str
    provider: str
    model: Optional[str] = None
    latency_ms: Optional[int] = None

class BaseConnector:
    name: str = "base"

    def generate_reply(self, prompt: str, user: str = "User") -> AIConnectorResult:
        raise NotImplementedError

class LocalEchoConnector(BaseConnector):
    """
    ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§ã‚‚å¿…ãšå‹•ä½œã™ã‚‹ãƒ­ãƒ¼ã‚«ãƒ«å‘ã‘ç°¡æ˜“ã‚³ãƒã‚¯ã‚¿ã€‚
    """
    name = "local-echo"

    def generate_reply(self, prompt: str, user: str = "User") -> AIConnectorResult:
        t0 = time.time()
        # ç°¡æ˜“ãƒ«ãƒ¼ãƒ«ï¼šç©ºãªã‚‰ä½•ã‚‚ã—ãªã„ã€çŸ­æ–‡ã¯ç›¸æ§Œã€è³ªå•ã£ã½ã„ãªã‚‰çŸ­å›ç­”
        text = prompt.strip()
        if not text:
            out = "ï¼ˆâ€¦ï¼‰"
        elif text.endswith("ï¼Ÿ") or text.endswith("?"):
            out = "ã„ã„è³ªå•ã ã­ã€‚ã–ã£ãã‚Šè¨€ã†ã¨â€•â€•" + text.rstrip("ï¼Ÿ?") + "ã¸ã®ç­”ãˆã¯ã€çŠ¶æ³æ¬¡ç¬¬ã ã‚ˆã€‚"
        elif len(text) <= 12:
            out = f"ã†ã‚“ã†ã‚“ã€{text}ã€‚"
        else:
            out = f"{user}ã•ã‚“ã€{text}ã«ã¤ã„ã¦ã¯æŠŠæ¡ã—ãŸã‚ˆã€‚è¦ç‚¹ã‚’çŸ­ãè¿”ã™ã­ã€‚"
        return AIConnectorResult(text=out, provider=self.name, model=None,
                                 latency_ms=int((time.time() - t0) * 1000))

class GeminiConnector(BaseConnector):
    """
    GEMINI_API_KEY ãŒå­˜åœ¨ã™ã‚‹ã¨ãã ã‘æœ‰åŠ¹åŒ–ã€‚

    v17.5.1: is_mock ãƒ•ãƒ©ã‚°è¿½åŠ 
    - is_mock=True: ãƒ¢ãƒƒã‚¯å®Ÿè£…ï¼ˆä»®æƒ³å¿œç­”ã®ã¿ï¼‰â†’ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ‰±ã„
    - is_mock=False: å®Ÿéš›ã®APIå®Ÿè£… â†’ çµ±åˆæ¸ˆã¿AIæ‰±ã„

    v17.5.2: æœ¬ç‰©ã® Gemini API å®Ÿè£…
    - google.generativeai ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨
    - ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒãªã„å ´åˆã¯è‡ªå‹•çš„ã«ãƒ¢ãƒƒã‚¯ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    """
    name = "gemini"

    def __init__(self, api_key: Optional[str], model: Optional[str] = None, is_mock: bool = False, timeout_seconds: int = 15):
        self.api_key = (api_key or "").strip()
        self.model = (model or os.getenv("GEMINI_MODEL") or "gemini-2.5-flash").strip()
        self.enabled = bool(self.api_key)
        self.timeout_seconds = timeout_seconds

        # âœ… v17.5.2: google.generativeai ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®åˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯
        self._genai_available = False
        self._genai_model = None

        if self.enabled and not is_mock:
            try:
                import google.generativeai as genai
                genai.configure(api_key=self.api_key)
                self._genai_model = genai.GenerativeModel(self.model)
                self._genai_available = True
                logger.info(f"âœ… Gemini API åˆæœŸåŒ–æˆåŠŸ: model={self.model}")
            except ImportError:
                logger.warning("âš ï¸ google-generativeai ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ¢ãƒƒã‚¯å®Ÿè£…ã§å‹•ä½œã—ã¾ã™ã€‚")
                logger.warning("   ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install google-generativeai")
                self._genai_available = False
            except Exception as e:
                logger.warning(f"âš ï¸ Gemini API åˆæœŸåŒ–å¤±æ•—: {e}ã€‚ãƒ¢ãƒƒã‚¯å®Ÿè£…ã§å‹•ä½œã—ã¾ã™ã€‚")
                self._genai_available = False

        # is_mock ãƒ•ãƒ©ã‚°ã‚’å®Ÿæ…‹ã«åˆã‚ã›ã¦æ›´æ–°
        self.is_mock = is_mock or not self._genai_available

    def generate_reply(self, prompt: str, user: str = "User") -> AIConnectorResult:
        t0 = time.time()

        if not self.enabled:
            # ç„¡åŠ¹æ™‚ã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ»ã‚¨ã‚³ãƒ¼é¢¨ã®ä¿é™º
            out = f"[Geminiç„¡åŠ¹] {user}ã•ã‚“ã€{prompt}ï¼ˆâ€»APIã‚­ãƒ¼æœªè¨­å®šã®ãŸã‚ãƒ­ãƒ¼ã‚«ãƒ«å¿œç­”ï¼‰"
            return AIConnectorResult(text=out, provider=self.name, model=self.model,
                                     latency_ms=int((time.time() - t0) * 1000))

        # âœ… v17.5.2: æœ¬ç‰©ã® Gemini API å‘¼ã³å‡ºã—
        if self._genai_available and self._genai_model:
            try:
                # Gemini API ã‚’ä½¿ã£ãŸå®Ÿéš›ã®å¿œç­”ç”Ÿæˆï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šã‚ã‚Šï¼‰
                response = self._genai_model.generate_content(
                    prompt,
                    request_options={"timeout": self.timeout_seconds}
                )
                result_text = response.text
                latency = int((time.time() - t0) * 1000)

                logger.info(f"âœ… Gemini API å¿œç­”æˆåŠŸ: latency={latency}ms, text={result_text[:50]}...")

                return AIConnectorResult(
                    text=result_text,
                    provider=self.name,
                    model=self.model,
                    latency_ms=latency
                )
            except Exception as e:
                # APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼æ™‚ã¯ä¾‹å¤–ã‚’æŠ•ã’ã¦ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã«ä»»ã›ã‚‹
                # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚ºã—ã¦æ©Ÿå¯†æƒ…å ±ã‚’é™¤å»
                safe_msg = _sanitize_error_message(str(e))
                logger.error(f"âŒ Gemini API å‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {safe_msg}")
                raise RuntimeError(f"Gemini API error: {safe_msg}")

        # ãƒ¢ãƒƒã‚¯å®Ÿè£…ï¼ˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒãªã„å ´åˆã‚„æ˜ç¤ºçš„ã«is_mock=Trueã®å ´åˆï¼‰
        out = f"[Geminiä»®æƒ³å¿œç­”] {user}ã•ã‚“ã€ã€Œ{prompt}ã€ã«ã¤ã„ã¦ã–ã£ãã‚Šè£œè¶³ã™ã‚‹ã­ã€‚"
        return AIConnectorResult(text=out, provider=self.name, model=self.model,
                                 latency_ms=int((time.time() - t0) * 1000))


# =========================================================
# AIIntegrationManager v17.3.1 çµ±åˆç‰ˆ
# =========================================================
class AIIntegrationManager:
    """
    OneComme/Chat â†’ AI â†’ Chat/Voice ã¸ã®å°ç·šã‚’æ‹…å½“ã™ã‚‹çµ±åˆãƒãƒãƒ¼ã‚¸ãƒ£ã€‚
    - å…¥å£: AI_REQUEST / ONECOMME_COMMENT / CHAT_MESSAGE
    - å‡ºå£: AI_RESPONSE / CHAT_APPEND / ï¼ˆä»»æ„ï¼‰VOICE_REQUEST
    - è¿½åŠ : AI_STATUS_REQUEST / AI_TEST_REQUEST ã«å¿œç­”ã—ã¦ AI_STATUS_UPDATE ã‚’è¿”ã™
    """

    def __init__(self, message_bus=None, config_manager=None):
        import os

        logger.debug(f"ğŸ› [DEBUG] AIIntegrationManager.__init__ é–‹å§‹: ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ID={id(self)}")

        # --- åŸºæœ¬ä¾å­˜ã®å–å¾— ---
        self.bus = message_bus or _get_bus()
        self.config = config_manager or _get_config()
        self._subs: list = []
        self._started = False

        # --- ç’°å¢ƒ/è¨­å®šã®èª­è¾¼ ---
        # ãƒ»primary/fallback ã¯è¨­å®š > ç’°å¢ƒå¤‰æ•°ã®é †ã§æ¡ç”¨
        primary = (self.config.get('ai.primary', None) or os.getenv('AI_PRIMARY', 'gemini')).strip().lower()
        fallback = (self.config.get('ai.fallback', None) or os.getenv('AI_FALLBACK', 'local-echo')).strip().lower()
        self.provider_primary = primary
        self.provider_fallback = fallback

        # ãƒ¢ãƒ‡ãƒ«ãƒ»APIã‚­ãƒ¼ãƒ»ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
        gemini_api = (os.getenv('GEMINI_API_KEY', '') or '').strip()
        model = (os.getenv('GEMINI_MODEL', '') or '').strip() or self.config.get('ai.model', 'gemini-2.5-flash')
        self.model_name = model

        # Gemini API ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ15ç§’ï¼‰
        timeout_seconds = 15
        if self.config:
            try:
                timeout_seconds = int(self.config.get("ai.gemini.timeout", 15))
            except Exception:
                timeout_seconds = 15

        # --- ã‚³ãƒã‚¯ã‚¿ã®ç”¨æ„ï¼ˆã‚µãƒãƒ¼ãƒˆå¤–ã¯ãƒ­ãƒ¼ã‚«ãƒ«Echoã«å¸åï¼‰ ---
        supported = {'gemini', 'local-echo', 'echo'}
        if self.provider_primary not in supported:
            self.provider_primary = 'gemini'
        if self.provider_fallback not in supported:
            self.provider_fallback = 'local-echo'

        # Primary
        self.connector_primary = None
        if self.provider_primary == 'gemini':
            self.connector_primary = GeminiConnector(gemini_api, model=model, timeout_seconds=timeout_seconds)

            # google-generativeai æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ™‚ã® UI é€šçŸ¥
            if self.connector_primary.is_mock and gemini_api:
                try:
                    import google.generativeai
                except ImportError:
                    error_msg = "âš ï¸ google-generativeai ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\nã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install google-generativeai"
                    logger.warning(error_msg)
                    if self.bus:
                        try:
                            self.bus.publish(_ev('AI_ERROR'), {
                                'message': 'google-generativeai ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ã™',
                                'detail': 'pip install google-generativeai ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„',
                                'severity': 'warning'
                            }, sender='ai_integration')
                        except Exception:
                            pass

        # Fallbackï¼ˆå¿…ãšå­˜åœ¨ã•ã›ã‚‹ï¼‰
        self.connector_fallback = LocalEchoConnector()

        # æœ‰åŠ¹ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ã‚’æ±ºå®šï¼ˆprimaryå„ªå…ˆï¼‰
        # âœ… v17.5.2: is_mock=True ã®å ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ‰±ã„
        self.active_provider = None
        if (
            self.connector_primary
            and getattr(self.connector_primary, 'enabled', False)
            and not getattr(self.connector_primary, 'is_mock', False)
        ):
            self.active_provider = self.provider_primary
        else:
            self.active_provider = 'local-echo'

        self.connected = bool(self.active_provider)

        logger.info(
            f"ğŸ¤– AIIntegrationManager æº–å‚™å®Œäº†: primary={self.provider_primary}, "
            f"fallback={self.provider_fallback}, active={self.active_provider}, model={self.model_name}"
        )

    # ---------- ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ« ----------
    def start(self):
        if self._started:
            return
        self._subscribe_bus()
        self._started = True
        self._status('AIIntegrationManager started')

    def stop(self):
        if not self._started:
            return
        self.cleanup()
        self._started = False
        self._status('AIIntegrationManager stopped')

    def cleanup(self):
        try:
            if hasattr(self.bus, 'unsubscribe'):
                for token in list(self._subs):
                    try:
                        # token ãŒ (ev, cb) å½¢å¼ or ãƒˆãƒ¼ã‚¯ãƒ³å½¢å¼ã©ã¡ã‚‰ã«ã‚‚å¯¾å¿œ
                        if isinstance(token, tuple) and len(token) == 2:
                            ev, cb = token
                            self.bus.unsubscribe(ev, cb)
                        else:
                            self.bus.unsubscribe(token)
                    except Exception:
                        pass
            self._subs.clear()
        except Exception as e:
            logger.debug(f'cleanup error: {e}')

    # ---------- Busè³¼èª­ ----------
    def _subscribe_bus(self):
        logger.debug(f"ğŸ› [DEBUG] AIIntegrationManager._subscribe_bus é–‹å§‹: ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ID={id(self)}")

        def sub(ev_name: str, fn):
            try:
                ev = _ev(ev_name)
                token = self.bus.subscribe(ev, fn)
                self._subs.append(token if token is not None else (ev, fn))
                logger.info(f'ğŸ“¡ subscribe: {ev_name}')
            except Exception as e:
                logger.warning(f'âš ï¸ subscribe å¤±æ•—: {ev_name}: {e}')

        # v17.3.1 æœ€å°å›è·¯ã®å…¥å£ã‚’ã™ã¹ã¦è³¼èª­
        # âŒ v17.3.1: AI_REQUEST ä»¥å¤–ã®ç›´æ¥è³¼èª­ã‚’å‰Šé™¤ï¼ˆtab_chat ã§ AI_REQUEST ã«ä¸€æœ¬åŒ–ï¼‰
        sub('AI_REQUEST',        self._on_ai_request)
        # sub('ONECOMME_COMMENT',  self._on_incoming_text)  # â† tab_chat ãŒ AI_REQUEST ã«å¤‰æ›
        # sub('CHAT_MESSAGE',      self._on_incoming_text)  # â† tab_chat ãŒ AI_REQUEST ã«å¤‰æ›
        # ãƒ¡ã‚¤ãƒ³èµ·å‹•åˆå›³ï¼ˆçµ±åˆåº¦ã‚’ä¸Šã’ã‚‹ãƒ•ãƒƒã‚¯ï¼‰
        sub('APP_STARTED',       self._on_app_started)
        # æ¥ç¶šçŠ¶æ…‹ç…§ä¼š/ãƒ†ã‚¹ãƒˆ
        sub('AI_STATUS_REQUEST', self._on_ai_status_request)
        sub('AI_TEST_REQUEST',   self._on_ai_test_request)

    # ---------- ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹é€ä¿¡ãƒ˜ãƒ«ãƒ‘ ----------
    def _send_status_update(self, reason: str = 'manual'):
        """
        AI_STATUS_UPDATE ã‚’ Bus ã«ç™ºè¡Œã™ã‚‹ã€‚

        v17.5: å®Ÿéš›ã®å‹•ä½œãƒ¢ãƒ¼ãƒ‰ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ¤å®šï¼‰ã‚’è¿½åŠ 
        - is_fallback: å®Ÿéš›ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œã—ã¦ã„ã‚‹ã‹
        - provider/model: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®å ´åˆã¯ 'fallback' / 'local-echo' ã«å¤‰æ›´

        v17.5.1: is_mock ãƒã‚§ãƒƒã‚¯è¿½åŠ 
        - primary connector ãŒ is_mock=True ã®å ´åˆã€is_fallback=True ã¨ã—ã¦æ‰±ã†
        - ã“ã‚Œã«ã‚ˆã‚Šä»®æƒ³å¿œç­”ã®ã¿ã®é–“ã¯ã€Œãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€ã¨ã—ã¦æ­£ã—ãè¡¨ç¤ºã•ã‚Œã‚‹
        """
        try:
            import os
            provider = self.active_provider or self.provider_primary or 'gemini'
            model = self.model_name or os.getenv('GEMINI_MODEL', 'gemini-2.5-flash')
            primary = self.connector_primary
            fallback = self.connector_fallback

            has_api_key = bool(getattr(primary, 'enabled', False) or os.getenv('GEMINI_API_KEY', '').strip())
            connector_available = bool(
                (primary and getattr(primary, 'enabled', False))
                or fallback
            )

            # âœ… v17.5: å®Ÿéš›ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã‹ã©ã†ã‹ã‚’åˆ¤å®š
            is_fallback = (self.active_provider in ['local-echo', 'echo', 'fallback', None])

            # âœ… v17.5.1: primary connector ãŒ is_mock=True ã®å ´åˆã‚‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ‰±ã„
            if primary and getattr(primary, 'is_mock', False):
                is_fallback = True

            # âœ… v17.5: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®å ´åˆã¯ provider/model ã‚‚å®Ÿæ…‹ã«åˆã‚ã›ã‚‹
            if is_fallback:
                provider = 'fallback'
                model = 'local-echo'

            payload = {
                'provider': provider,
                'model': model,
                'has_api_key': has_api_key,
                'connector_available': connector_available,
                'is_fallback': is_fallback,  # âœ… v17.5: å®Ÿæ…‹ãƒ•ãƒ©ã‚°è¿½åŠ 
                'reason': reason,
            }

            ev = _ev('AI_STATUS_UPDATE')
            self.bus.publish(ev, payload, sender='AIIntegrationManager')
            logger.info(f'ğŸ“¡ AI_STATUS_UPDATE é€ä¿¡: provider={provider}, model={model}, has_api_key={has_api_key}, '
                       f'connector_available={connector_available}, is_fallback={is_fallback}, reason={reason}')

        except Exception as e:
            logger.warning(f'âš ï¸ AI_STATUS_UPDATE é€ä¿¡å¤±æ•—: {e}')

    # ---------- å—ä¿¡ãƒãƒ³ãƒ‰ãƒ© ----------
    def _on_app_started(self, data: Optional[dict], sender=None):
        """
        APP_STARTED ã‚¤ãƒ™ãƒ³ãƒˆã‚’å—ä¿¡ã—ãŸã‚‰ã€åˆå›ã® AI_STATUS_UPDATE ã‚’è‡ªå‹•ç™ºè¡Œã™ã‚‹ã€‚
        ã“ã‚Œã«ã‚ˆã‚Šã€å„ã‚¿ãƒ–ãŒå€‹åˆ¥ã« AI_STATUS_REQUEST ã‚’ç™ºè¡Œã™ã‚‹å¿…è¦ãŒãªããªã‚‹ã€‚
        """
        self._status('APP_STARTED å—ä¿¡ï¼ˆAIIntegration æœ‰åŠ¹ï¼‰')
        # Phase 1.3.1: åˆå› AIçŠ¶æ…‹é€šçŸ¥ã‚’è‡ªå‹•ç™ºè¡Œ
        logger.info("ğŸ“¡ APP_STARTED å—ä¿¡ â†’ AI_STATUS_UPDATE ã‚’è‡ªå‹•ç™ºè¡Œã—ã¾ã™")
        self._send_status_update(reason='app_started')

    def _on_ai_status_request(self, data: Optional[dict], sender=None):
        logger.info(f'[AI_STATUS_REQUEST] sender={sender}, data={data}')
        self._send_status_update(reason='status_request')

    def _on_ai_test_request(self, data: Optional[dict], sender=None):
        """
        AIæ¥ç¶šãƒ†ã‚¹ãƒˆè¦æ±‚ï¼ˆAI_TEST_REQUESTï¼‰ã‚’å‡¦ç†ã™ã‚‹ã€‚
        å®Ÿéš›ã®æ¥ç¶šå†åˆæœŸåŒ–ã¯è¡Œã‚ãšã€ç¾åœ¨ã®çŠ¶æ…‹ã‚’ AI_STATUS_UPDATE ã¨ã—ã¦è¿”ã™ç°¡æ˜“ä»•æ§˜ã€‚
        """
        logger.info(f'[AI_TEST_REQUEST] sender={sender}, data={data}')
        # å¿…è¦ãªã‚‰ã“ã“ã§ç°¡å˜ãªè‡ªå·±è¨ºæ–­ã‚’è¿½åŠ ã—ã¦ã‚‚ã‚ˆã„ãŒã€
        # ã¾ãšã¯ç¾åœ¨çŠ¶æ…‹ã‚’ãã®ã¾ã¾é€šçŸ¥ã™ã‚‹ã€‚
        self._send_status_update(reason='test')

    def _on_ai_request(self, data: Optional[dict], sender: Optional[str] = None):
        """
        AI_REQUEST ã‚’å—ä¿¡ã—ã¦ AI_RESPONSE / VOICE_REQUEST ã‚’ç™ºè¡Œã™ã‚‹ä¸­æ ¸ãƒ­ã‚¸ãƒƒã‚¯ã€‚

        v17.5.2: ã‚­ãƒ£ãƒ©è¨­å®šã‚’ä½¿ã£ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
        v17.5.3: UIãƒ•ãƒªãƒ¼ã‚ºé˜²æ­¢ã®ãŸã‚ã€AIç”Ÿæˆå‡¦ç†ã‚’åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œ

        æœŸå¾…ã•ã‚Œã‚‹ data å½¢å¼ä¾‹:
            {
                "text": "...",
                "username": "ãƒ¦ãƒ¼ã‚¶ãƒ¼å",
                "user": "ãƒ¦ãƒ¼ã‚¶ãƒ¼åï¼ˆæ—§ä»•æ§˜ï¼‰",
                "provider": "gemini" ãªã©,
                "model": "gemini-2.5-flash" ãªã©,
                "system_prompt": "...",  # v17.5.2: ã‚­ãƒ£ãƒ©è¨­å®š
                "personality": "...",
                "ai_name": "...",
                "age": "...",
                "speaking_style": "...",
                "background": "..."
            }
        """
        try:
            payload = data or {}

            # ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’æŠ½å‡º
            text = payload.get("text") or ""
            user = payload.get("username") or payload.get("user") or "ãƒ¦ãƒ¼ã‚¶ãƒ¼"

            if not text:
                logger.info("ğŸ›ˆ AI_REQUEST: text ãŒç©ºã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                return

            # âœ… v17.5.2: ã‚­ãƒ£ãƒ©è¨­å®šã‚’å–å¾—
            system_prompt = payload.get("system_prompt", "")
            personality = payload.get("personality", "")
            ai_name = payload.get("ai_name", "ãã‚…ã‚‹ã‚‹")
            age = payload.get("age", "")
            speaking_style = payload.get("speaking_style", "")
            background = payload.get("background", "")
            response_length_limit = payload.get("response_length_limit", 200)

            # ãƒ—ãƒ­ãƒã‚¤ãƒ€ã¨ãƒ¢ãƒ‡ãƒ«ã‚’æ±ºå®š
            provider = (
                payload.get("provider")
                or self.active_provider
                or self.provider_primary
                or "gemini"
            )
            model = payload.get("model") or self.model_name or "gemini-2.5-flash"

            # âœ… å®›å…ˆãƒ¦ãƒ¼ã‚¶ãƒ¼åï¼ˆå…ƒã‚³ãƒ¡ãƒ³ãƒˆã®é€ã‚Šä¸»ï¼‰ã‚’æŠ½å‡º
            original_username = (
                payload.get("original_username")
                or payload.get("username")
                or payload.get("user")
                or user
            )

            # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°
            logger.info(
                "ğŸ› [DEBUG] AI_REQUEST å—ä¿¡: sender=%s, provider=%s, model=%s, "
                "user=%s, text=%s..., ai_name=%s, response_limit=%sæ–‡å­—",
                sender,
                provider,
                model,
                user,
                text[:30],
                ai_name,
                response_length_limit,
            )

            # âœ… v17.5.3: UIãƒ•ãƒªãƒ¼ã‚ºé˜²æ­¢ã®ãŸã‚ã€AIç”Ÿæˆå‡¦ç†ã‚’åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œ
            import threading
            worker = threading.Thread(
                target=self._process_ai_request_async,
                args=(text, user, system_prompt, personality, ai_name, age, speaking_style, background, response_length_limit, provider, model, original_username),
                daemon=True,
            )
            worker.start()
            logger.info("ğŸ§µ AIç”Ÿæˆå‡¦ç†ã‚’åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§é–‹å§‹ã—ã¾ã—ãŸ")

        except Exception as e:
            logger.exception(f"AI_REQUEST processing error: {e}")
            self._error(f"AI_REQUEST error: {e}")

    def _process_ai_request_async(
        self,
        text: str,
        user: str,
        system_prompt: str,
        personality: str,
        ai_name: str,
        age: str,
        speaking_style: str,
        background: str,
        response_length_limit: int,
        provider: str,
        model: str,
        original_username: str = "",
    ):
        """
        AIç”Ÿæˆå‡¦ç†ã‚’éåŒæœŸã§å®Ÿè¡Œã™ã‚‹å†…éƒ¨ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆåˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œã•ã‚Œã‚‹ï¼‰ã€‚

        v17.5.3: UIãƒ•ãƒªãƒ¼ã‚ºé˜²æ­¢ã®ãŸã‚ã€_on_ai_request() ã‹ã‚‰åˆ†é›¢
        """
        try:
            # âœ… v17.5.2: ã‚­ãƒ£ãƒ©è¨­å®šã‚’ä½¿ã£ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
            full_prompt = self._build_prompt_with_character(
                text=text,
                user=user,
                system_prompt=system_prompt,
                personality=personality,
                ai_name=ai_name,
                age=age,
                speaking_style=speaking_style,
                background=background,
                response_length_limit=response_length_limit
            )

            logger.debug(f"ğŸ§© æ§‹ç¯‰ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {full_prompt[:200]}...")

            # å®Ÿéš›ã®AIç”Ÿæˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ããƒ­ã‚¸ãƒƒã‚¯ã«ä¸€å…ƒåŒ–ï¼‰
            result = self._generate_with_fallback(
                full_prompt,
                user=user,
                provider=provider,
                model=model,
            )

            # AI_RESPONSE / VOICE_REQUEST ã® publish ã¯ã“ã“ã«ä¸€å…ƒåŒ–
            # âœ… original_username ã¯é–¢æ•°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦æ—¢ã«å—ã‘å–ã£ã¦ã„ã‚‹
            # âœ… v17.6+: ai_name ã‚‚æ¸¡ã—ã¦ã€ãƒãƒ£ãƒƒãƒˆè¡¨ç¤ºã§ã‚­ãƒ£ãƒ©åã‚’æ­£ã—ãè¡¨ç¤º
            self._emit_ai_result(
                result,
                user=user,
                original_username=original_username,
                ai_name=ai_name,
            )

        except Exception as e:
            logger.exception(f"éåŒæœŸAIå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            self._error(f"AIå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")


    # âŒ v17.3.1: ONECOMME_COMMENT / CHAT_MESSAGE ã®ç›´æ¥è³¼èª­ã‚’å»ƒæ­¢ã—ãŸãŸã‚ã€ã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¯æœªä½¿ç”¨
    # def _on_incoming_text(self, data: Optional[dict], sender=None):
    #     """
    #     ONECOMME_COMMENT / CHAT_MESSAGE ã‹ã‚‰ã®å…¥åŠ›ã‚’çµ±ä¸€å‡¦ç†ã€‚
    #     æ¡ä»¶æ¬¡ç¬¬ã§ AI_REQUEST ã«ãƒ–ãƒªãƒƒã‚¸ã™ã‚‹ä½™åœ°ã‚’æ®‹ã™ã€‚
    #     """
    #     try:
    #         if not isinstance(data, dict):
    #             return
    #         text = (data.get('text') or '').strip()
    #         if not text:
    #             return
    #         # å°†æ¥çš„ã«ã€Œãã‚…ã‚‹ã‚‹å‘¼ã³ã‹ã‘ã€æ¤œå‡ºãªã©ã‚’ã“ã“ã«å®Ÿè£…ã™ã‚‹æƒ³å®šã€‚
    #     except Exception:
    #         return

    # ---------- å†…éƒ¨ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯ ----------
    def _build_prompt_with_character(
        self,
        text: str,
        user: str,
        system_prompt: str = "",
        personality: str = "",
        ai_name: str = "ãã‚…ã‚‹ã‚‹",
        age: str = "",
        speaking_style: str = "",
        background: str = "",
        response_length_limit: int = 200
    ) -> str:
        """
        ã‚­ãƒ£ãƒ©è¨­å®šã‚’åæ˜ ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ã™ã‚‹ã€‚

        v17.5.2: AIã‚­ãƒ£ãƒ©è¨­å®šã‚¿ãƒ–ã®æ€§æ ¼ã‚’AIå¿œç­”ã«åæ˜ ã•ã›ã‚‹
        v17 Refactor: å¿œç­”æ–‡å­—æ•°åˆ¶é™ã‚’å®Œå…¨å®Ÿè£…

        ãƒ‡ãƒ¼ã‚¿ã®æµã‚Œ:
        1. tab_chat/app.py ã® _do_ai_request() ãŒ UnifiedConfigManager ã‹ã‚‰è¨­å®šã‚’å–å¾—
           - ai_personality.basic_info.name / personality / age / speaking_style / background
           - ai.response_length_limit
        2. AI_REQUEST ã‚¤ãƒ™ãƒ³ãƒˆã® payload ã«å…¨è¨­å®šã‚’åŸ‹ã‚è¾¼ã‚€
        3. ã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã§å—ã‘å–ã£ãŸè¨­å®šã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«æ§‹ç¯‰
        4. Gemini / ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯AI ã«é€ä¿¡

        Args:
            text: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
            user: ãƒ¦ãƒ¼ã‚¶ãƒ¼å
            system_prompt: ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆã‚­ãƒ£ãƒ©è¨­å®šã®åŸºæœ¬ï¼‰
            personality: æ€§æ ¼è¨­å®š
            ai_name: AIã®åå‰
            age: å¹´é½¢
            speaking_style: å£èª¿
            background: èƒŒæ™¯ãƒ»è¨­å®š
            response_length_limit: å¿œç­”æ–‡å­—æ•°åˆ¶é™ï¼ˆç›®å®‰ã€AIãƒ¢ãƒ‡ãƒ«ã«æŒ‡ç¤ºã¨ã—ã¦é€ã‚‹ï¼‰

        Returns:
            æ§‹ç¯‰ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆGeminiã«é€ä¿¡ã™ã‚‹æœ€çµ‚å½¢ï¼‰
        """
        parts = []

        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆæœ€å„ªå…ˆãƒ»æ—¢ã«å®Œå…¨ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å ´åˆãŒã‚ã‚‹ï¼‰
        # v17.5.3: system_promptãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã€ãã‚Œã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æœ€å°é™ã®è¿½åŠ ã®ã¿
        if system_prompt:
            parts.append(system_prompt)
            # system_promptã«æ—¢ã«è¨­å®šãŒå«ã¾ã‚Œã¦ã„ã‚‹ãŸã‚ã€é‡è¤‡ã‚’é¿ã‘ã‚‹
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®ã¿ã‚’è¿½åŠ 
            parts.append(f"\n{user}ã•ã‚“ã‹ã‚‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {text}")
            return "\n".join(parts)

        # âŒ ä»¥ä¸‹ã¯ system_prompt ãŒãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆé€šå¸¸ã¯ä½¿ç”¨ã•ã‚Œãªã„ï¼‰
        # v17.5.3: system_prompt ãŒå¸¸ã« tab_ai_unified ã‹ã‚‰ä¾›çµ¦ã•ã‚Œã‚‹ãŸã‚ã€
        # ã“ã®éƒ¨åˆ†ã¯ã»ã¼å®Ÿè¡Œã•ã‚Œã¾ã›ã‚“

        # åŸºæœ¬æƒ…å ±ï¼ˆæœ€å°é™ï¼‰
        if ai_name:
            parts.append(f"ã‚ãªãŸã¯{ai_name}ã§ã™ã€‚")

        # æ€§æ ¼ã¨å£èª¿ï¼ˆç°¡æ½”ã«ï¼‰
        character_info = []
        if personality:
            character_info.append(f"æ€§æ ¼: {personality}")
        if speaking_style:
            character_info.append(f"å£èª¿: {speaking_style}")
        if character_info:
            parts.append("ã€".join(character_info))

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›
        parts.append(f"\n{user}ã•ã‚“ã‹ã‚‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {text}")

        # æŒ‡ç¤ºï¼ˆç°¡æ½”ã«ï¼‰
        parts.append(f"\nä¸Šè¨˜ã®è¨­å®šã§ã€è‡ªç„¶ã«å¿œç­”ã—ã¦ãã ã•ã„ã€‚")

        # âœ… v17.5.2: å¿œç­”æ–‡å­—æ•°åˆ¶é™ã®æŒ‡ç¤º
        if response_length_limit and response_length_limit > 0:
            parts.append(f"å¿œç­”ã¯{response_length_limit}æ–‡å­—ç¨‹åº¦ã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚")

        return "\n".join(parts)

    def _generate_with_fallback(self, prompt: str, user: str, provider: str, model: str) -> AIConnectorResult:
        """
        Phase 3: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é †åºã«åŸºã¥ã„ã¦AIå¿œç­”ã‚’ç”Ÿæˆã™ã‚‹

        UnifiedConfigManager ã‹ã‚‰ primary_provider ã¨ fallback_providers ã‚’èª­ã¿è¾¼ã¿ã€
        é †ç•ªã«è©¦è¡Œã™ã‚‹ã€‚å…¨ã¦å¤±æ•—ã—ãŸã‚‰å›ºå®šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™ã€‚

        è©¦è¡Œé †åº:
        1. primary_provider (è¨­å®šã‹ã‚‰å–å¾—)
        2. fallback_providers (è¨­å®šã®ãƒªã‚¹ãƒˆã‹ã‚‰é †ç•ªã«)
        3. æœ€å¾Œã®æ‰‹æ®µ: local-echoï¼ˆå›ºå®šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼‰

        Args:
            prompt: AI ã«é€ä¿¡ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            user: ãƒ¦ãƒ¼ã‚¶ãƒ¼å
            provider: ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§æŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ­ãƒã‚¤ãƒ€ï¼ˆå„ªå…ˆï¼‰
            model: ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«

        Returns:
            AIConnectorResult: AIå¿œç­”çµæœ
        """
        # Phase 3: UnifiedConfigManager ã‹ã‚‰è¨­å®šã‚’å–å¾—
        primary = None
        fallbacks = []

        if self.config and hasattr(self.config, 'get'):
            # æ–°è¨­å®šã‚’å„ªå…ˆï¼ˆPhase 3ï¼‰
            primary = self.config.get('ai.primary_provider', None)
            fallbacks = self.config.get('ai.fallback_providers', None)

            # æ—§è¨­å®šã¸ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆäº’æ›æ€§ï¼‰
            if not primary:
                primary = self.config.get('ai.provider_primary', None) or self.config.get('ai.provider', None)
            if not fallbacks:
                old_fallback = self.config.get('ai.provider_fallback', None)
                fallbacks = [old_fallback] if old_fallback else []

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        if not primary:
            primary = self.provider_primary or 'gemini'
        if not fallbacks or not isinstance(fallbacks, list):
            fallbacks = ['local-echo']

        # è©¦è¡Œé †åºã‚’æ§‹ç¯‰
        # 1) ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§æŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ­ãƒã‚¤ãƒ€ãŒã‚ã‚Œã°æœ€å„ªå…ˆ
        # 2) primary_provider
        # 3) fallback_providersï¼ˆãƒªã‚¹ãƒˆã®é †ç•ªé€šã‚Šï¼‰
        providers_to_try = []
        if provider and provider not in providers_to_try:
            providers_to_try.append(provider)
        if primary and primary not in providers_to_try:
            providers_to_try.append(primary)
        for fb in fallbacks:
            if fb and fb not in providers_to_try:
                providers_to_try.append(fb)

        # å®‰å…¨ç­–: local-echo ãŒå«ã¾ã‚Œã¦ã„ãªã„å ´åˆã¯æœ€å¾Œã«è¿½åŠ 
        if 'local-echo' not in providers_to_try:
            providers_to_try.append('local-echo')

        logger.info(f"ğŸ”„ Phase 3 ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é †åº: {providers_to_try}")

        # å„ãƒ—ãƒ­ãƒã‚¤ãƒ€ã‚’é †ç•ªã«è©¦è¡Œ
        last_err = None
        for idx, p in enumerate(providers_to_try):
            try:
                logger.info(f"ğŸ”„ [{idx+1}/{len(providers_to_try)}] ãƒ—ãƒ­ãƒã‚¤ãƒ€ '{p}' ã‚’è©¦è¡Œä¸­...")

                # ãƒ—ãƒ­ãƒã‚¤ãƒ€ã”ã¨ã®å‡¦ç†
                if p == 'gemini':
                    # Geminiã‚³ãƒã‚¯ã‚¿ã®ãƒã‚§ãƒƒã‚¯
                    if not self.connector_primary:
                        raise RuntimeError('Gemini connector not initialized')
                    if not getattr(self.connector_primary, 'enabled', False):
                        raise RuntimeError('Gemini disabled (API key missing)')

                    # APIã‚­ãƒ¼ã®ç¢ºèª
                    import os
                    api_key = os.getenv('GEMINI_API_KEY', '').strip()
                    if not api_key:
                        raise RuntimeError('GEMINI_API_KEY not set')

                    # Gemini API å‘¼ã³å‡ºã—
                    result = self.connector_primary.generate_reply(prompt, user=user)
                    logger.info(f"âœ… ãƒ—ãƒ­ãƒã‚¤ãƒ€ '{p}' ã§å¿œç­”æˆåŠŸ")
                    return result

                elif p in ('local-echo', 'echo', 'fallback'):
                    # ãƒ­ãƒ¼ã‚«ãƒ«ã‚¨ã‚³ãƒ¼ï¼ˆå¿…ãšæˆåŠŸã™ã‚‹ï¼‰
                    if not self.connector_fallback:
                        raise RuntimeError('Local-echo connector not initialized')
                    result = self.connector_fallback.generate_reply(prompt, user=user)
                    logger.info(f"âœ… ãƒ—ãƒ­ãƒã‚¤ãƒ€ '{p}' ã§å¿œç­”æˆåŠŸï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰")
                    return result

                elif p == 'gpt4all':
                    # GPT4Allï¼ˆå°†æ¥å®Ÿè£…ï¼‰
                    raise RuntimeError('GPT4All not implemented yet')

                else:
                    # æœªçŸ¥ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€
                    raise RuntimeError(f'Unknown provider: {p}')

            except Exception as e:
                last_err = e
                logger.warning(f"âš ï¸ ãƒ—ãƒ­ãƒã‚¤ãƒ€ '{p}' ã§å¤±æ•—: {e}")
                continue

        # å…¨ã¦å¤±æ•—ã—ãŸå ´åˆã®æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆå›ºå®šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼‰
        logger.error(f'âŒ å…¨ã¦ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ã§å¤±æ•—ã—ã¾ã—ãŸã€‚æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚æœ€å¾Œã®ã‚¨ãƒ©ãƒ¼: {last_err}')

        # ç·Šæ€¥ç”¨ã®å›ºå®šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™
        emergency_text = f"ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€ç¾åœ¨AIã‚µãƒ¼ãƒ“ã‚¹ã«æ¥ç¶šã§ãã¾ã›ã‚“ã€‚ã—ã°ã‚‰ãã—ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚ï¼ˆã‚¨ãƒ©ãƒ¼: {last_err}ï¼‰"
        return AIConnectorResult(
            text=emergency_text,
            provider='emergency-fallback',
            model='none',
            latency_ms=0
        )

    @staticmethod
    def _extract_speakable_part(text: str) -> str:
        """
        AIå¿œç­”ã‹ã‚‰èª­ã¿ä¸Šã’ç”¨ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹ã€‚

        ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«å…¨ä½“ã§ã¯ãªãã€å®Ÿéš›ã«ä¼šè©±ã§ç™ºè¨€ã—ã¦ã„ã‚‹éƒ¨åˆ†ã ã‘ã‚’æŠ½å‡ºã™ã‚‹ã€‚

        ãƒ«ãƒ¼ãƒ«:
        1. "---" ãŒã‚ã‚Œã°ã€æœ€å¾Œã® "---" ä»¥é™ã‚’èª­ã¿ä¸Šã’å¯¾è±¡ã¨ã™ã‚‹
        2. ãã‚Œã‚‚ãªã‘ã‚Œã°ã€ç©ºè¡Œã§åŒºåˆ‡ã£ã¦æœ€å¾Œã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’ä½¿ã†
        3. æœ€ä½é™ã€"## ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«" ãªã©ã®è¦‹å‡ºã—ã¯é™¤å¤–ã™ã‚‹

        Args:
            text: AIå¿œç­”ã®å…¨æ–‡

        Returns:
            èª­ã¿ä¸Šã’ã«é©ã—ãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        if not text or not text.strip():
            return ""

        # 1) "---" ãŒã‚ã‚Œã°ã€æœ€å¾Œã®åŒºåˆ‡ã‚Šä»¥é™ã‚’ä½¿ã†
        if "---" in text:
            parts = text.split("---")
            candidate = parts[-1].strip()
            if candidate:
                logger.debug(f"ğŸ”§ [speakable] '---'åŒºåˆ‡ã‚Šã§æŠ½å‡º: {candidate[:50]}...")
                return candidate

        # 2) ç©ºè¡Œã§åŒºåˆ‡ã£ã¦æœ€å¾Œã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’ä½¿ã†
        blocks = [b.strip() for b in text.split("\n\n") if b.strip()]
        if blocks:
            # è¦‹å‡ºã—ï¼ˆ## ã§å§‹ã¾ã‚‹è¡Œï¼‰ã‚’å«ã‚€ãƒ–ãƒ­ãƒƒã‚¯ã¯ã‚¹ã‚­ãƒƒãƒ—
            for block in reversed(blocks):
                if not block.startswith("##") and not block.startswith("#"):
                    logger.debug(f"ğŸ”§ [speakable] ç©ºè¡ŒåŒºåˆ‡ã‚Šã§æŠ½å‡º: {block[:50]}...")
                    return block

        # 3) ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãã®ã¾ã¾ï¼ˆãŸã ã—ãƒ­ã‚°ã«è­¦å‘Šï¼‰
        logger.warning(f"âš ï¸ [speakable] æŠ½å‡ºå¤±æ•—ã€å…¨æ–‡ã‚’ä½¿ç”¨: {text[:50]}...")
        return text.strip()

    def _emit_ai_result(
        self,
        result: AIConnectorResult,
        user: str,
        original_username: Optional[str] = None,
        ai_name: str = "ãã‚…ã‚‹ã‚‹",
    ):
        """
        AIç”Ÿæˆçµæœã‚’ AI_RESPONSE ã¨ VOICE_REQUEST ã¨ã—ã¦ç™ºè¡Œã™ã‚‹ã€‚

        v17.5.4: èª­ã¿ä¸Šã’ç”¨ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºæ©Ÿèƒ½è¿½åŠ 
        - full_text: ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ï¼‹ä¼šè©±æ–‡å…¨ä½“ï¼ˆAI_RESPONSEç”¨ï¼‰
        - speakable_text: èª­ã¿ä¸Šã’ã«é©ã—ãŸéƒ¨åˆ†ã®ã¿ï¼ˆVOICE_REQUESTç”¨ï¼‰
        v17.6+: ai_name ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¿½åŠ ï¼ˆã‚­ãƒ£ãƒ©åˆ¥è¡¨ç¤ºå¯¾å¿œï¼‰
        """
        full_text = result.text
        speakable_text = self._extract_speakable_part(full_text)

        # å®›å…ˆãƒ¦ãƒ¼ã‚¶ãƒ¼åï¼ˆNone ã®å ´åˆã¯ user ã‚’æ¡ç”¨ï¼‰
        if not original_username:
            original_username = user

        logger.info(f"ğŸ”§ [DEBUG] _emit_ai_result() é–‹å§‹")
        logger.info(f"ğŸ”§ [DEBUG]   full_text: {full_text[:80]}...")
        logger.info(f"ğŸ”§ [DEBUG]   speakable_text: {speakable_text[:80]}...")
        logger.info(f"ğŸ”§ [DEBUG]   user={user}, provider={result.provider}")
        logger.info(f"ğŸ”§ [DEBUG]   original_username={original_username}")
        logger.info(f"ğŸ”§ [DEBUG]   ai_name={ai_name}")

        # AI_RESPONSE ã«ã¯å…¨æ–‡ã‚’æ¸¡ã™ï¼ˆãƒãƒ£ãƒƒãƒˆè¡¨ç¤ºç”¨ï¼‰
        payload = {
            "text": full_text,
            "user": user,
            "original_username": original_username,  # âœ… ã“ã“ã§è¼‰ã›ã‚‹
            "ai_name": ai_name,  # âœ… v17.6+: å®Ÿéš›ã«å¿œç­”ã—ãŸã‚­ãƒ£ãƒ©å
            "provider": result.provider,
            "model": result.model,
            "latency_ms": result.latency_ms,
            "ts": time.time(),
        }
        logger.info(f"ğŸ“¢ AI_RESPONSEç™ºè¡Œæº–å‚™å®Œäº†: text={full_text[:50]}..., user={user}")
        self._pub('AI_RESPONSE', payload)
        logger.info(f"âœ… AI_RESPONSEç™ºè¡Œå®Œäº†")

        # âœ… v17.3.1: VOICE_REQUEST ã¯ AIIntegrationManager ãŒå”¯ä¸€ã®ç™ºè¡Œå…ƒï¼ˆãƒ«ãƒ¼ãƒ«ãƒ–ãƒƒã‚¯æº–æ‹ ï¼‰
        # v17.5.4: èª­ã¿ä¸Šã’ç”¨ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’ä½¿ç”¨
        # v17.6.0: ãƒ­ãƒ¼ãƒ«åˆ¥ã‚­ãƒ£ãƒ©é¸æŠå¯¾å¿œ - AIå¿œç­”ã«ã¯ role='ai' ã‚’è¿½åŠ 
        # âœ… v17.6.1: voice.read.ai è¨­å®šãƒã‚§ãƒƒã‚¯è¿½åŠ 
        try:
            # âœ… v17.6.1è¿½åŠ : voice.read.ai è¨­å®šã‚’ãƒã‚§ãƒƒã‚¯
            voice_read_ai_enabled = True  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: æœ‰åŠ¹
            if self.config and hasattr(self.config, 'get'):
                voice_read_ai_enabled = bool(self.config.get('voice.read.ai', True))

            if voice_read_ai_enabled:  # âœ… v17.6.1è¿½åŠ : æ¡ä»¶åˆ¤å®š
                voice_payload = {
                    'text': speakable_text,  # â† èª­ã¿ä¸Šã’ç”¨ãƒ†ã‚­ã‚¹ãƒˆã®ã¿
                    'username': user,
                    'source': 'ai_response',
                    'priority': 50,
                    'role': 'ai',  # âœ… v17.6.0: AIã‚­ãƒ£ãƒ©ã®ãƒ­ãƒ¼ãƒ«ã‚’æŒ‡å®š
                }
                logger.info(f"ğŸ¤ VOICE_REQUESTç™ºè¡Œæº–å‚™: text={speakable_text[:80]}..., username={user}, role=ai")
                logger.info(f"ğŸ”§ [DEBUG] AIIntegrationManager: MessageBusã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ ID={id(self.bus)}")
                self._pub('VOICE_REQUEST', voice_payload)
                logger.info(f'âœ… VOICE_REQUESTç™ºè¡Œå®Œäº†ï¼ˆAIIntegrationManagerï¼‰')
            else:  # âœ… v17.6.1è¿½åŠ 
                logger.info(f'â„¹ï¸ voice.read.ai ãŒç„¡åŠ¹ã®ãŸã‚ VOICE_REQUEST ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™')
        except Exception as e:
            logger.error(f'âŒ VOICE_REQUEST ç™ºè¡Œã‚¨ãƒ©ãƒ¼: {e}', exc_info=True)

    # ---------- Busãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ----------
    def _pub(self, ev_name: str, data: Optional[dict] = None):
        try:
            ev = _ev(ev_name)
            logger.debug(f"ğŸ“¤ [DEBUG] publishé–‹å§‹: event={ev_name}, data_keys={list((data or {}).keys())}")
            self.bus.publish(ev, data or {}, sender='ai_integration')
            logger.debug(f"ğŸ“¤ [DEBUG] publishå®Œäº†: event={ev_name}")
        except Exception as e:
            logger.error(f'âŒ publish error: {ev_name}: {e}', exc_info=True)

    def _status(self, msg: str):
        logger.info(msg)
        try:
            self.bus.publish(_ev('STATUS_UPDATE'), {'kind': 'ai', 'message': msg}, sender='ai_integration')
        except Exception:
            pass

    def _error(self, msg: str):
        # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚ºã—ã¦æ©Ÿå¯†æƒ…å ±ã‚’é™¤å»
        safe_msg = _sanitize_error_message(msg)
        logger.error(safe_msg)
        try:
            self.bus.publish(_ev('AI_ERROR'), {'message': safe_msg, 'ts': time.time()}, sender='ai_integration')
        except Exception:
            pass


# === Backward-compat wrapper (bus/config ã‚­ãƒ¼å¸å) ===
try:
    _AIIM_Original = AIIntegrationManager
    class AIIntegrationManager(_AIIM_Original):  # type: ignore
        def __init__(self, *args, **kwargs):
            if 'message_bus' not in kwargs and 'bus' in kwargs:
                kwargs['message_bus'] = kwargs.pop('bus')
            if 'config_manager' not in kwargs and 'config' in kwargs:
                kwargs['config_manager'] = kwargs.pop('config')
            super().__init__(*args, **kwargs)
except Exception:
    pass


# ---------------------------------------------------------
# ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³å®Ÿè¡Œï¼ˆç°¡æ˜“ãƒ†ã‚¹ãƒˆï¼‰
# ---------------------------------------------------------
if __name__ == "__main__":
    mgr = AIIntegrationManager()
    mgr.start()
    mgr.bus.publish("AI_REQUEST", {"text": "ãƒ†ã‚¹ãƒˆã§ã™ã€‚å°ç·šãƒã‚§ãƒƒã‚¯ï¼", "user": "Tester"})
    time.sleep(0.3)
    logger.info("âœ… selftest å®Œäº†ï¼ˆãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼‰")
